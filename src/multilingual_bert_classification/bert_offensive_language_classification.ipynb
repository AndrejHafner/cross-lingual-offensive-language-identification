{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "offensive_language_classification(1).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cyVZJzmAq4uX",
        "HisMvSuImxRK",
        "1LkLfhwd4AgD",
        "j6YNMf3w4AgJ"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyVZJzmAq4uX"
      },
      "source": [
        "# Environment preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ar-aTG7f__VR",
        "outputId": "0b96b5ff-a913-4058-a119-ed93670f59e1"
      },
      "source": [
        "# Install the required library\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bPUQ_ml6rMM",
        "outputId": "dd3c27ba-e129-41be-d65a-a76c57bf86a6"
      },
      "source": [
        "# Mount the Google Drive folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63yHbtwXD0ox",
        "outputId": "fe26091e-60af-427d-83f1-e5012f85c0b1"
      },
      "source": [
        "# Handle all the required imports\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import nltk\n",
        "import datetime\n",
        "\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, \\\n",
        "    get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler, TensorDataset\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW62n98OerR7"
      },
      "source": [
        "### **Methods used for BERT training and testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-mF9v417Niw"
      },
      "source": [
        "# Functions required for training and testing\n",
        "\n",
        "def get_torch_device():\n",
        "    # Check for GPU...\n",
        "    if torch.cuda.is_available():\n",
        "        print('GPU:', torch.cuda.get_device_name(0))\n",
        "        return torch.device(\"cuda\")\n",
        "\n",
        "    else:\n",
        "        print('No GPU available, using the CPU instead.')\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def convert_to_input(contents, tokenizer, max_length=128, pad_token=0, pad_token_segment_id=0):\n",
        "    input_ids, attention_masks, token_type_ids = [], [], []\n",
        "\n",
        "    for sentence in tqdm(contents, position=0, leave=True):\n",
        "        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=max_length)\n",
        "\n",
        "        i, t = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
        "        m = [1] * len(i)\n",
        "\n",
        "        padding_length = max_length - len(i)\n",
        "\n",
        "        i = i + ([pad_token] * padding_length)\n",
        "        m = m + ([0] * padding_length)\n",
        "        t = t + ([pad_token_segment_id] * padding_length)\n",
        "\n",
        "        input_ids.append(torch.Tensor([i]))\n",
        "        attention_masks.append(torch.Tensor([m]))\n",
        "        token_type_ids.append(torch.Tensor([t]))\n",
        "\n",
        "    return torch.cat(input_ids, dim=0).to(torch.int64), torch.cat(attention_masks, dim=0).to(torch.int64)\n",
        "\n",
        "def get_dataloaders(train_dataset, val_dataset, batch_size):\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,  # The training samples.\n",
        "        sampler=RandomSampler(train_dataset),  # Select batches randomly\n",
        "        batch_size=batch_size  # Trains with this batch size.\n",
        "    )\n",
        "\n",
        "    val_dataloader = DataLoader(\n",
        "        val_dataset,  # The validation samples.\n",
        "        sampler=SequentialSampler(val_dataset),  # Pull out batches sequentially.\n",
        "        batch_size=batch_size  # Evaluate with this batch size.\n",
        "    )\n",
        "\n",
        "    return train_dataloader, val_dataloader\n",
        "\n",
        "\n",
        "def train(model, optimizer, scheduler, train_dataloader, validation_dataloader, epochs, device, multiclass=False):\n",
        "    # Set the seed value all over the place to make this reproducible.\n",
        "    seed_val = 42\n",
        "\n",
        "    random.seed(seed_val)\n",
        "    np.random.seed(seed_val)\n",
        "    torch.manual_seed(seed_val)\n",
        "    torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "    training_stats = []\n",
        "\n",
        "    # Measure the total training time for the whole run.\n",
        "    total_t0 = time.time()\n",
        "\n",
        "    # For each epoch...\n",
        "    for epoch_i in range(0, epochs):\n",
        "        # Perform one full pass over the training set.\n",
        "\n",
        "        print(\"\")\n",
        "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "        print('Training...')\n",
        "\n",
        "        # Measure how long the training epoch takes.\n",
        "        t0 = time.time()\n",
        "\n",
        "        # ========================================\n",
        "        #               Train\n",
        "        # ========================================\n",
        "\n",
        "        # Reset the total loss for this epoch.\n",
        "        total_train_loss = 0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "            # Progress update every 40 batches.\n",
        "            if step % 40 == 0 and not step == 0:\n",
        "                # Calculate elapsed time in minutes.\n",
        "                elapsed = format_time(time.time() - t0)\n",
        "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass (evaluate the model on this training batch).\n",
        "            # The documentation for this `model` function is here:\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # It returns different\n",
        "            model_output = model(b_input_mask\n",
        "                                 )\n",
        "            model_output = model(b_input_ids,\n",
        "                                 token_type_ids=None,\n",
        "                                 attention_mask=b_input_mask,\n",
        "                                 labels=b_labels)\n",
        "\n",
        "            loss = model_output.loss\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate the gradients.\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "        # Calculate the average loss over all of the batches.\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "        # Measure how long this epoch took.\n",
        "        training_time = format_time(time.time() - t0)\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "        # ========================================\n",
        "        #               Validation\n",
        "        # ========================================\n",
        "        # After the completion of each training epoch, measure our performance on\n",
        "        # our validation set.\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Running Validation...\")\n",
        "\n",
        "        t0 = time.time()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        total_eval_accuracy = 0\n",
        "        total_eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "\n",
        "        # Evaluate data for one epoch\n",
        "        val_predictions = []\n",
        "        val_labels = []\n",
        "        for batch in validation_dataloader:\n",
        "            # Unpack this training batch from our dataloader\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "\n",
        "            # Tell pytorch not to bother with constructing the compute graph during\n",
        "            # the forward pass, since this is only needed for backprop (training).\n",
        "            with torch.no_grad():\n",
        "\n",
        "                eval_outputs = model(b_input_ids,\n",
        "                                       token_type_ids=None,\n",
        "                                       attention_mask=b_input_mask,\n",
        "                                       labels=b_labels)\n",
        "                loss = eval_outputs.loss\n",
        "                logits = eval_outputs.logits\n",
        "\n",
        "            # Accumulate the validation loss.\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "            # Move logits and labels to CPU\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "            # Calculate the accuracy for this batch of test sentences, and\n",
        "            # accumulate it over all batches.\n",
        "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "            pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "            labels_flat = label_ids.flatten()\n",
        "\n",
        "            val_predictions.append(pred_flat)\n",
        "            val_labels.append(labels_flat)\n",
        "\n",
        "        val_predictions = np.hstack(np.array(val_predictions))\n",
        "        val_labels = np.hstack(np.array(val_labels))\n",
        "\n",
        "        precision = precision_score(val_labels, val_predictions, average=(\"micro\" if multiclass else \"binary\"))\n",
        "        recall = recall_score(val_labels, val_predictions, average=(\"micro\" if multiclass else \"binary\"))\n",
        "        f1score = f1_score(val_labels, val_predictions, average=(\"micro\" if multiclass else \"binary\"))\n",
        "\n",
        "        # Report the final accuracy for this validation run.\n",
        "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "        print(f\"f1-score: {round(f1score, 2)}, precision: {round(precision,2)}, recall: {round(recall, 2)}\")\n",
        "        # Calculate the average loss over all of the batches.\n",
        "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "        # Measure how long the validation run took.\n",
        "        validation_time = format_time(time.time() - t0)\n",
        "\n",
        "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "        print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "        # Record all statistics from this epoch.\n",
        "        training_stats.append(\n",
        "            {\n",
        "                'epoch': epoch_i + 1,\n",
        "                'Training Loss': avg_train_loss,\n",
        "                'Valid. Loss': avg_val_loss,\n",
        "                'Valid. Accur.': avg_val_accuracy,\n",
        "                'Training Time': training_time,\n",
        "                'Validation Time': validation_time\n",
        "            }\n",
        "        )\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0)))\n",
        "    return model.state_dict()\n",
        "\n",
        "def test(dataloader, model, score_average=\"binary\"):\n",
        "\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            eval_outputs = model(b_input_ids,\n",
        "                                  token_type_ids=None,\n",
        "                                  attention_mask=b_input_mask,\n",
        "                                  labels=b_labels)\n",
        "        logits = eval_outputs.logits\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Store predictions and true labels\n",
        "        predictions.append(logits)\n",
        "        true_labels.append(label_ids)\n",
        "\n",
        "    predictions = np.hstack(predictions)\n",
        "    true_labels = np.hstack(true_labels)\n",
        "\n",
        "    precision = precision_score(true_labels, predictions, average=score_average)\n",
        "    recall = recall_score(true_labels, predictions, average=score_average)\n",
        "    f1score = f1_score(true_labels, predictions, average=score_average)\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "\n",
        "    print(f\"accuracy: {accuracy}, f1-score: {f1score}, precision: {precision}, recall: {recall}\")\n",
        "\n",
        "    return accuracy, f1score, precision, recall\n",
        "\n",
        "def prepare_training_dataloaders(X, y, tokenizer, batch_size, max_length, device):\n",
        "    x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_input_ids, train_attention_masks = convert_to_input(x_train, tokenizer, max_length=max_length)\n",
        "    val_input_ids, val_attention_masks = convert_to_input(x_val, tokenizer, max_length=max_length)\n",
        "\n",
        "    y_train = torch.tensor(y_train).to(torch.int64)\n",
        "    y_val = torch.tensor(y_val).to(torch.int64)\n",
        "\n",
        "    train_dataset = TensorDataset(train_input_ids, train_attention_masks, y_train)\n",
        "    val_dataset = TensorDataset(val_input_ids, val_attention_masks, y_val)\n",
        "\n",
        "    train_dataloader, val_dataloader = get_dataloaders(train_dataset, val_dataset, batch_size)\n",
        "    return train_dataloader, val_dataloader\n",
        "\n",
        "def prepare_test_dataloader(X, y, tokenizer, batch_size, max_length, device):\n",
        "    test_input_ids, test_attention_masks = convert_to_input(X, tokenizer, max_length=max_length)\n",
        "\n",
        "    y_test = torch.tensor(y).to(torch.int64)\n",
        "\n",
        "    test_dataset = TensorDataset(test_input_ids, test_attention_masks, y_test)\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,  # The validation samples.\n",
        "        sampler=SequentialSampler(test_dataset),  # Pull out batches sequentially.\n",
        "        batch_size=32  # Evaluate with this batch size.\n",
        "    )\n",
        "\n",
        "    return test_dataloader\n",
        "\n",
        "\n",
        "def get_bert_eng(num_labels=2, pretrained_weights_path=None):\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-uncased\",\n",
        "        num_labels=num_labels,\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False,\n",
        "    )\n",
        "\n",
        "    if pretrained_weights_path != None:\n",
        "      model.load_state_dict(torch.load(pretrained_weights_path))\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_bert_crosloen(model_folder_path, num_labels=2, pretrained_weights_path=None):\n",
        "    model = BertForSequenceClassification.from_pretrained(model_folder_path, num_labels=num_labels)\n",
        "    if pretrained_weights_path != None:\n",
        "      model.load_state_dict(torch.load(pretrained_weights_path))\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpG7yg5TeNJp"
      },
      "source": [
        "# **Multilingual BERT training on ENG datasets and testing on ENG & SLO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_I2vD-2lym-"
      },
      "source": [
        "## Binary datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcZ7DjNsgAk0"
      },
      "source": [
        "### Gab training and testing on ENG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrKT5tR97p_J",
        "outputId": "f45d29c9-0304-4381-8b0b-d9498462227a"
      },
      "source": [
        "# Training\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 2e-5\n",
        "epochs = 3\n",
        "max_length = 128\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/offensive_language_classification/final_datasets/gab/train.csv\")# (17870 false, 146001 true)\n",
        "comments = df[\"content\"].values\n",
        "labels = df[\"type\"].values\n",
        "\n",
        "# Get Pytorch device\n",
        "device = get_torch_device()\n",
        "\n",
        "# Prepare the training and validation data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "train_dataloader, val_dataloader = prepare_training_dataloaders(comments, labels, tokenizer, batch_size, max_length, device)\n",
        "\n",
        "# Get the model and move it to GPU\n",
        "model = get_bert_crosloen(\"/content/drive/MyDrive/offensive_language_classification/crosloen_bert/\")\n",
        "model.cuda()\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr=learning_rate,  # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps=1e-8  # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(train_dataloader) * epochs)\n",
        "\n",
        "state_dict = train(model, optimizer, scheduler, train_dataloader, val_dataloader, epochs, device)\n",
        "torch.save(state_dict, \"/content/drive/MyDrive/offensive_language_classification/final_models/crosloen_gab_trained.pth\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: Tesla T4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/20783 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 20783/20783 [00:15<00:00, 1325.09it/s]\n",
            "100%|██████████| 5196/5196 [00:03<00:00, 1337.73it/s]\n",
            "Some weights of the model checkpoint at /content/drive/MyDrive/offensive_language_classification/crosloen_bert/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/offensive_language_classification/crosloen_bert/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    650.    Elapsed: 0:00:33.\n",
            "  Batch    80  of    650.    Elapsed: 0:01:07.\n",
            "  Batch   120  of    650.    Elapsed: 0:01:41.\n",
            "  Batch   160  of    650.    Elapsed: 0:02:17.\n",
            "  Batch   200  of    650.    Elapsed: 0:02:53.\n",
            "  Batch   240  of    650.    Elapsed: 0:03:29.\n",
            "  Batch   280  of    650.    Elapsed: 0:04:05.\n",
            "  Batch   320  of    650.    Elapsed: 0:04:41.\n",
            "  Batch   360  of    650.    Elapsed: 0:05:18.\n",
            "  Batch   400  of    650.    Elapsed: 0:05:55.\n",
            "  Batch   440  of    650.    Elapsed: 0:06:32.\n",
            "  Batch   480  of    650.    Elapsed: 0:07:09.\n",
            "  Batch   520  of    650.    Elapsed: 0:07:46.\n",
            "  Batch   560  of    650.    Elapsed: 0:08:23.\n",
            "  Batch   600  of    650.    Elapsed: 0:09:00.\n",
            "  Batch   640  of    650.    Elapsed: 0:09:37.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0:09:46\n",
            "\n",
            "Running Validation...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:204: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:205: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Accuracy: 0.91\n",
            "f1-score: 0.9, precision: 0.95, recall: 0.86\n",
            "  Validation Loss: 0.27\n",
            "  Validation took: 0:00:39\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    650.    Elapsed: 0:00:37.\n",
            "  Batch    80  of    650.    Elapsed: 0:01:14.\n",
            "  Batch   120  of    650.    Elapsed: 0:01:52.\n",
            "  Batch   160  of    650.    Elapsed: 0:02:29.\n",
            "  Batch   200  of    650.    Elapsed: 0:03:06.\n",
            "  Batch   240  of    650.    Elapsed: 0:03:43.\n",
            "  Batch   280  of    650.    Elapsed: 0:04:20.\n",
            "  Batch   320  of    650.    Elapsed: 0:04:58.\n",
            "  Batch   360  of    650.    Elapsed: 0:05:35.\n",
            "  Batch   400  of    650.    Elapsed: 0:06:12.\n",
            "  Batch   440  of    650.    Elapsed: 0:06:49.\n",
            "  Batch   480  of    650.    Elapsed: 0:07:26.\n",
            "  Batch   520  of    650.    Elapsed: 0:08:04.\n",
            "  Batch   560  of    650.    Elapsed: 0:08:41.\n",
            "  Batch   600  of    650.    Elapsed: 0:09:18.\n",
            "  Batch   640  of    650.    Elapsed: 0:09:56.\n",
            "\n",
            "  Average training loss: 0.23\n",
            "  Training epoch took: 0:10:05\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.92\n",
            "f1-score: 0.91, precision: 0.93, recall: 0.89\n",
            "  Validation Loss: 0.24\n",
            "  Validation took: 0:00:40\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    650.    Elapsed: 0:00:38.\n",
            "  Batch    80  of    650.    Elapsed: 0:01:15.\n",
            "  Batch   120  of    650.    Elapsed: 0:01:53.\n",
            "  Batch   160  of    650.    Elapsed: 0:02:31.\n",
            "  Batch   200  of    650.    Elapsed: 0:03:08.\n",
            "  Batch   240  of    650.    Elapsed: 0:03:46.\n",
            "  Batch   280  of    650.    Elapsed: 0:04:24.\n",
            "  Batch   320  of    650.    Elapsed: 0:05:02.\n",
            "  Batch   360  of    650.    Elapsed: 0:05:40.\n",
            "  Batch   400  of    650.    Elapsed: 0:06:17.\n",
            "  Batch   440  of    650.    Elapsed: 0:06:55.\n",
            "  Batch   480  of    650.    Elapsed: 0:07:33.\n",
            "  Batch   520  of    650.    Elapsed: 0:08:10.\n",
            "  Batch   560  of    650.    Elapsed: 0:08:48.\n",
            "  Batch   600  of    650.    Elapsed: 0:09:26.\n",
            "  Batch   640  of    650.    Elapsed: 0:10:04.\n",
            "\n",
            "  Average training loss: 0.19\n",
            "  Training epoch took: 0:10:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.92\n",
            "f1-score: 0.92, precision: 0.93, recall: 0.9\n",
            "  Validation Loss: 0.25\n",
            "  Validation took: 0:00:40\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:32:03 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwtkYpGkiIYZ",
        "outputId": "686099ee-d3d4-4a84-8148-8b3783f62894"
      },
      "source": [
        "# Testing\n",
        "max_length = 128\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/offensive_language_classification/final_datasets/gab/test.csv\")# (17870 false, 146001 true)\n",
        "comments = df[\"content\"].values\n",
        "labels = df[\"type\"].values\n",
        "\n",
        "# Get Pytorch device\n",
        "device = get_torch_device()\n",
        "\n",
        "# Prepare the training and validation data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "test_dataloader = prepare_test_dataloader(comments, labels, tokenizer, batch_size, max_length, device)\n",
        "\n",
        "# Get the model and move it to GPU\n",
        "model = get_bert_crosloen(\"/content/drive/MyDrive/offensive_language_classification/crosloen_bert/\", \n",
        "                          pretrained_weights_path=\"/content/drive/MyDrive/offensive_language_classification/final_models/crosloen_gab_trained.pth\")\n",
        "model.cuda()\n",
        "\n",
        "# Run test\n",
        "test(test_dataloader, model, score_average=\"macro\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: Tesla T4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/6495 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 6495/6495 [00:04<00:00, 1344.71it/s]\n",
            "Some weights of the model checkpoint at /content/drive/MyDrive/offensive_language_classification/crosloen_bert/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/offensive_language_classification/crosloen_bert/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.9173210161662817, f1-score: 0.9157962759555511, precision: 0.9178922997840422, recall: 0.9142042403381438\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9173210161662817,\n",
              " 0.9157962759555511,\n",
              " 0.9178922997840422,\n",
              " 0.9142042403381438)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwuxjr8OjFR1"
      },
      "source": [
        "### Reddit training and testing on ENG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6beFBsdkQcA",
        "outputId": "251a14ab-32e2-494c-cd4c-16bcfb9db72f"
      },
      "source": [
        "# Training\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 2e-5\n",
        "epochs = 3\n",
        "max_length = 128\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/offensive_language_classification/final_datasets/reddit/train.csv\")# (17870 false, 146001 true)\n",
        "comments = df[\"content\"].values\n",
        "labels = df[\"type\"].values\n",
        "\n",
        "# Get Pytorch device\n",
        "device = get_torch_device()\n",
        "\n",
        "# Prepare the training and validation data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "train_dataloader, val_dataloader = prepare_training_dataloaders(comments, labels, tokenizer, batch_size, max_length, device)\n",
        "\n",
        "# Get the model and move it to GPU\n",
        "model = get_bert_crosloen(\"/content/drive/MyDrive/offensive_language_classification/crosloen_bert/\")\n",
        "model.cuda()\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr=learning_rate,  # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps=1e-8  # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(train_dataloader) * epochs)\n",
        "\n",
        "state_dict = train(model, optimizer, scheduler, train_dataloader, val_dataloader, epochs, device)\n",
        "torch.save(state_dict, \"/content/drive/MyDrive/offensive_language_classification/final_models/crosloen_reddit_trained.pth\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: Tesla T4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/14214 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 14214/14214 [00:16<00:00, 865.68it/s]\n",
            "100%|██████████| 3554/3554 [00:04<00:00, 834.94it/s]\n",
            "Some weights of the model checkpoint at /content/drive/MyDrive/offensive_language_classification/crosloen_bert/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/offensive_language_classification/crosloen_bert/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    445.    Elapsed: 0:00:38.\n",
            "  Batch    80  of    445.    Elapsed: 0:01:17.\n",
            "  Batch   120  of    445.    Elapsed: 0:01:55.\n",
            "  Batch   160  of    445.    Elapsed: 0:02:33.\n",
            "  Batch   200  of    445.    Elapsed: 0:03:11.\n",
            "  Batch   240  of    445.    Elapsed: 0:03:49.\n",
            "  Batch   280  of    445.    Elapsed: 0:04:27.\n",
            "  Batch   320  of    445.    Elapsed: 0:05:05.\n",
            "  Batch   360  of    445.    Elapsed: 0:05:43.\n",
            "  Batch   400  of    445.    Elapsed: 0:06:21.\n",
            "  Batch   440  of    445.    Elapsed: 0:06:59.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0:07:03\n",
            "\n",
            "Running Validation...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:204: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:205: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Accuracy: 0.91\n",
            "f1-score: 0.79, precision: 0.82, recall: 0.76\n",
            "  Validation Loss: 0.28\n",
            "  Validation took: 0:00:28\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    445.    Elapsed: 0:00:38.\n",
            "  Batch    80  of    445.    Elapsed: 0:01:16.\n",
            "  Batch   120  of    445.    Elapsed: 0:01:54.\n",
            "  Batch   160  of    445.    Elapsed: 0:02:32.\n",
            "  Batch   200  of    445.    Elapsed: 0:03:10.\n",
            "  Batch   240  of    445.    Elapsed: 0:03:48.\n",
            "  Batch   280  of    445.    Elapsed: 0:04:26.\n",
            "  Batch   320  of    445.    Elapsed: 0:05:04.\n",
            "  Batch   360  of    445.    Elapsed: 0:05:42.\n",
            "  Batch   400  of    445.    Elapsed: 0:06:20.\n",
            "  Batch   440  of    445.    Elapsed: 0:06:58.\n",
            "\n",
            "  Average training loss: 0.27\n",
            "  Training epoch took: 0:07:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91\n",
            "f1-score: 0.8, precision: 0.83, recall: 0.77\n",
            "  Validation Loss: 0.29\n",
            "  Validation took: 0:00:28\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    445.    Elapsed: 0:00:38.\n",
            "  Batch    80  of    445.    Elapsed: 0:01:16.\n",
            "  Batch   120  of    445.    Elapsed: 0:01:54.\n",
            "  Batch   160  of    445.    Elapsed: 0:02:32.\n",
            "  Batch   200  of    445.    Elapsed: 0:03:10.\n",
            "  Batch   240  of    445.    Elapsed: 0:03:48.\n",
            "  Batch   280  of    445.    Elapsed: 0:04:26.\n",
            "  Batch   320  of    445.    Elapsed: 0:05:04.\n",
            "  Batch   360  of    445.    Elapsed: 0:05:42.\n",
            "  Batch   400  of    445.    Elapsed: 0:06:20.\n",
            "  Batch   440  of    445.    Elapsed: 0:06:58.\n",
            "\n",
            "  Average training loss: 0.23\n",
            "  Training epoch took: 0:07:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90\n",
            "f1-score: 0.78, precision: 0.77, recall: 0.79\n",
            "  Validation Loss: 0.29\n",
            "  Validation took: 0:00:27\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:22:30 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjBY7jzDk-aQ",
        "outputId": "e4439623-6698-4be1-a324-c24cdbfde852"
      },
      "source": [
        "# Testing\n",
        "max_length = 128\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/offensive_language_classification/final_datasets/reddit/test.csv\")# (17870 false, 146001 true)\n",
        "comments = df[\"content\"].values\n",
        "labels = df[\"type\"].values\n",
        "\n",
        "\n",
        "# Get Pytorch device\n",
        "device = get_torch_device()\n",
        "\n",
        "# Prepare the training and validation data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "test_dataloader = prepare_test_dataloader(comments, labels, tokenizer, batch_size, max_length, device)\n",
        "\n",
        "# Get the model and move it to GPU\n",
        "model = get_bert_crosloen(\"/content/drive/MyDrive/offensive_language_classification/crosloen_bert/\", \n",
        "                          pretrained_weights_path=\"/content/drive/MyDrive/offensive_language_classification/final_models/crosloen_reddit_trained.pth\")\n",
        "model.cuda()\n",
        "\n",
        "# Run test\n",
        "test(test_dataloader, model, score_average=\"macro\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: Tesla T4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/4442 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 4442/4442 [00:05<00:00, 783.31it/s]\n",
            "Some weights of the model checkpoint at /content/drive/MyDrive/offensive_language_classification/crosloen_bert/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/offensive_language_classification/crosloen_bert/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.9027465105808195, f1-score: 0.8684532141814179, precision: 0.8657578191579269, recall: 0.8712582452117336\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9027465105808195,\n",
              " 0.8684532141814179,\n",
              " 0.8657578191579269,\n",
              " 0.8712582452117336)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QPm4RKjnmnE"
      },
      "source": [
        "## Multi-class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMAdNHJEoGgY"
      },
      "source": [
        "### Kaggle toxic comment training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmG0SiThoDh8",
        "outputId": "8a64eee6-3475-4328-8e77-087a1038c2b6"
      },
      "source": [
        "# Training\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 2e-5\n",
        "epochs = 3\n",
        "max_length = 128\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/offensive_language_classification/final_datasets/toxic_comment_relabeled_multiclass/train.csv\")\n",
        "comments = df[\"content\"].values\n",
        "labels = df[\"type\"].values\n",
        "\n",
        "# Get Pytorch device\n",
        "device = get_torch_device()\n",
        "\n",
        "# Prepare the training and validation data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "train_dataloader, val_dataloader = prepare_training_dataloaders(comments, labels, tokenizer, batch_size, max_length, device)\n",
        "\n",
        "# Get the model and move it to GPU\n",
        "model = get_bert_crosloen(\"/content/drive/MyDrive/offensive_language_classification/crosloen_bert/\", num_labels=4)\n",
        "model.cuda()\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr=learning_rate,  # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps=1e-8  # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(train_dataloader) * epochs)\n",
        "\n",
        "state_dict = train(model, optimizer, scheduler, train_dataloader, val_dataloader, epochs, device, multiclass=True)\n",
        "torch.save(state_dict, \"/content/drive/MyDrive/offensive_language_classification/final_models/crosloen_toxic_comment_trained.pth\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: Tesla T4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/15121 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 15121/15121 [00:23<00:00, 655.58it/s]\n",
            "100%|██████████| 3781/3781 [00:05<00:00, 650.39it/s]\n",
            "Some weights of the model checkpoint at /content/drive/MyDrive/offensive_language_classification/crosloen_bert/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/offensive_language_classification/crosloen_bert/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    473.    Elapsed: 0:00:39.\n",
            "  Batch    80  of    473.    Elapsed: 0:01:17.\n",
            "  Batch   120  of    473.    Elapsed: 0:01:56.\n",
            "  Batch   160  of    473.    Elapsed: 0:02:34.\n",
            "  Batch   200  of    473.    Elapsed: 0:03:12.\n",
            "  Batch   240  of    473.    Elapsed: 0:03:50.\n",
            "  Batch   280  of    473.    Elapsed: 0:04:29.\n",
            "  Batch   320  of    473.    Elapsed: 0:05:07.\n",
            "  Batch   360  of    473.    Elapsed: 0:05:45.\n",
            "  Batch   400  of    473.    Elapsed: 0:06:23.\n",
            "  Batch   440  of    473.    Elapsed: 0:07:01.\n",
            "\n",
            "  Average training loss: 0.94\n",
            "  Training epoch took: 0:07:32\n",
            "\n",
            "Running Validation...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:204: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:205: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Accuracy: 0.64\n",
            "f1-score: 0.64, precision: 0.64, recall: 0.64\n",
            "  Validation Loss: 0.81\n",
            "  Validation took: 0:00:29\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    473.    Elapsed: 0:00:38.\n",
            "  Batch    80  of    473.    Elapsed: 0:01:16.\n",
            "  Batch   120  of    473.    Elapsed: 0:01:55.\n",
            "  Batch   160  of    473.    Elapsed: 0:02:33.\n",
            "  Batch   200  of    473.    Elapsed: 0:03:11.\n",
            "  Batch   240  of    473.    Elapsed: 0:03:49.\n",
            "  Batch   280  of    473.    Elapsed: 0:04:27.\n",
            "  Batch   320  of    473.    Elapsed: 0:05:06.\n",
            "  Batch   360  of    473.    Elapsed: 0:05:44.\n",
            "  Batch   400  of    473.    Elapsed: 0:06:22.\n",
            "  Batch   440  of    473.    Elapsed: 0:07:00.\n",
            "\n",
            "  Average training loss: 0.70\n",
            "  Training epoch took: 0:07:31\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.67\n",
            "f1-score: 0.67, precision: 0.67, recall: 0.67\n",
            "  Validation Loss: 0.74\n",
            "  Validation took: 0:00:29\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    473.    Elapsed: 0:00:38.\n",
            "  Batch    80  of    473.    Elapsed: 0:01:16.\n",
            "  Batch   120  of    473.    Elapsed: 0:01:55.\n",
            "  Batch   160  of    473.    Elapsed: 0:02:33.\n",
            "  Batch   200  of    473.    Elapsed: 0:03:11.\n",
            "  Batch   240  of    473.    Elapsed: 0:03:49.\n",
            "  Batch   280  of    473.    Elapsed: 0:04:27.\n",
            "  Batch   320  of    473.    Elapsed: 0:05:05.\n",
            "  Batch   360  of    473.    Elapsed: 0:05:44.\n",
            "  Batch   400  of    473.    Elapsed: 0:06:22.\n",
            "  Batch   440  of    473.    Elapsed: 0:07:00.\n",
            "\n",
            "  Average training loss: 0.58\n",
            "  Training epoch took: 0:07:31\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.67\n",
            "f1-score: 0.67, precision: 0.67, recall: 0.67\n",
            "  Validation Loss: 0.76\n",
            "  Validation took: 0:00:29\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:24:02 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7lOXsDCOSDi",
        "outputId": "0634b244-e0e6-4a14-ff78-c13511229b43"
      },
      "source": [
        "# Testing\n",
        "max_length = 128\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/offensive_language_classification/final_datasets/toxic_comment_relabeled_multiclass/test.csv\")# (17870 false, 146001 true)\n",
        "comments = df[\"content\"].values\n",
        "labels = df[\"type\"].values\n",
        "\n",
        "\n",
        "# Get Pytorch device\n",
        "device = get_torch_device()\n",
        "\n",
        "# Prepare the training and validation data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "test_dataloader = prepare_test_dataloader(comments, labels, tokenizer, batch_size, max_length, device)\n",
        "\n",
        "# Get the model and move it to GPU\n",
        "model = get_bert_crosloen(\"/content/drive/MyDrive/offensive_language_classification/crosloen_bert/\", \n",
        "                          pretrained_weights_path=\"/content/drive/MyDrive/offensive_language_classification/final_models/crosloen_toxic_comment_trained.pth\",\n",
        "                          num_labels=4)\n",
        "model.cuda()\n",
        "\n",
        "# Run test\n",
        "test(test_dataloader, model, score_average=\"macro\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: Tesla T4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/4823 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 4823/4823 [00:07<00:00, 682.69it/s]\n",
            "Some weights of the model checkpoint at /content/drive/MyDrive/offensive_language_classification/crosloen_bert/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/offensive_language_classification/crosloen_bert/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.7028820236367406, f1-score: 0.6060090681186833, precision: 0.6567773376674054, recall: 0.5860866947754766\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7028820236367406,\n",
              " 0.6060090681186833,\n",
              " 0.6567773376674054,\n",
              " 0.5860866947754766)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds8KxVXmYae6"
      },
      "source": [
        "### Trac2 Subtask A training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6ACfpwgYgOU",
        "outputId": "2c4475fa-cc94-483c-ef40-ae29e6a8ca62"
      },
      "source": [
        "# Training\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 2e-5\n",
        "epochs = 3\n",
        "max_length = 128\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/offensive_language_classification/final_datasets/trac2/train.csv\")\n",
        "comments = df[\"content\"].fillna(\"\").values\n",
        "labels = df[\"type\"].fillna(0).values\n",
        "\n",
        "# Get Pytorch device\n",
        "device = get_torch_device()\n",
        "\n",
        "# Prepare the training and validation data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "train_dataloader, val_dataloader = prepare_training_dataloaders(comments, labels, tokenizer, batch_size, max_length, device)\n",
        "\n",
        "# Get the model and move it to GPU\n",
        "model = get_bert_crosloen(\"/content/drive/MyDrive/offensive_language_classification/crosloen_bert/\", num_labels=3)\n",
        "model.cuda()\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr=learning_rate,  # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps=1e-8  # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(train_dataloader) * epochs)\n",
        "\n",
        "state_dict = train(model, optimizer, scheduler, train_dataloader, val_dataloader, epochs, device, multiclass=True)\n",
        "torch.save(state_dict, \"/content/drive/MyDrive/offensive_language_classification/final_models/crosloen_trac2_trained.pth\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: Tesla T4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/3410 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 3410/3410 [00:01<00:00, 1878.60it/s]\n",
            "100%|██████████| 853/853 [00:00<00:00, 1660.99it/s]\n",
            "Some weights of the model checkpoint at /content/drive/MyDrive/offensive_language_classification/crosloen_bert/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/offensive_language_classification/crosloen_bert/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    107.    Elapsed: 0:00:35.\n",
            "  Batch    80  of    107.    Elapsed: 0:01:12.\n",
            "\n",
            "  Average training loss: 0.63\n",
            "  Training epoch took: 0:01:38\n",
            "\n",
            "Running Validation...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:204: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:205: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Accuracy: 0.77\n",
            "f1-score: 0.77, precision: 0.77, recall: 0.77\n",
            "  Validation Loss: 0.61\n",
            "  Validation took: 0:00:07\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    107.    Elapsed: 0:00:37.\n",
            "  Batch    80  of    107.    Elapsed: 0:01:15.\n",
            "\n",
            "  Average training loss: 0.57\n",
            "  Training epoch took: 0:01:39\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "f1-score: 0.8, precision: 0.8, recall: 0.8\n",
            "  Validation Loss: 0.56\n",
            "  Validation took: 0:00:07\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    107.    Elapsed: 0:00:37.\n",
            "  Batch    80  of    107.    Elapsed: 0:01:15.\n",
            "\n",
            "  Average training loss: 0.50\n",
            "  Training epoch took: 0:01:40\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "f1-score: 0.8, precision: 0.8, recall: 0.8\n",
            "  Validation Loss: 0.56\n",
            "  Validation took: 0:00:07\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:05:16 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nybs3B5EZB3e",
        "outputId": "bee44c9a-0e57-4c66-e065-c9f744ab922e"
      },
      "source": [
        "# Testing\n",
        "max_length = 128\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/offensive_language_classification/final_datasets/trac2/test.csv\")# (17870 false, 146001 true)\n",
        "comments = df[\"content\"].values\n",
        "labels = df[\"type\"].values\n",
        "\n",
        "\n",
        "# Get Pytorch device\n",
        "device = get_torch_device()\n",
        "\n",
        "# Prepare the training and validation data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "test_dataloader = prepare_test_dataloader(comments, labels, tokenizer, batch_size, max_length, device)\n",
        "\n",
        "# Get the model and move it to GPU\n",
        "model = get_bert_crosloen(\"/content/drive/MyDrive/offensive_language_classification/crosloen_bert/\", \n",
        "                          pretrained_weights_path=\"/content/drive/MyDrive/offensive_language_classification/final_models/crosloen_trac2_trained.pth\",\n",
        "                          num_labels=3)\n",
        "model.cuda()\n",
        "\n",
        "# Run test\n",
        "test(test_dataloader, model, score_average=\"macro\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: Tesla T4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1066 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 1066/1066 [00:00<00:00, 1953.58it/s]\n",
            "Some weights of the model checkpoint at /content/drive/MyDrive/offensive_language_classification/crosloen_bert/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/offensive_language_classification/crosloen_bert/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.7870544090056285, f1-score: 0.3821143913941086, precision: 0.5395185716563048, recall: 0.38487472123835764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7870544090056285,\n",
              " 0.3821143913941086,\n",
              " 0.5395185716563048,\n",
              " 0.38487472123835764)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou9sjjYfZlgF"
      },
      "source": [
        "## Testing models on SLO data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4rqsgDnZhsZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db22708e-842e-43f1-a02b-29347d51a7a7"
      },
      "source": [
        "# Preparation for testing\n",
        "max_length = 128\n",
        "batch_size = 32\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/offensive_language_classification/final_datasets/slo-twitter-test.csv\")# (17870 false, 146001 true)\n",
        "comments = df[\"content\"].values\n",
        "labels = df[\"type\"].values\n",
        "\n",
        "\n",
        "# Get Pytorch device\n",
        "device = get_torch_device()\n",
        "\n",
        "# Prepare the training and validation data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "test_dataloader = prepare_test_dataloader(comments, labels, tokenizer, batch_size, max_length, device)\n",
        "\n",
        "labels_binary = np.minimum(labels, 1)\n",
        "test_dataloader_binary = prepare_test_dataloader(comments, labels_binary, tokenizer, batch_size, max_length, device)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: Tesla K80\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/18459 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 18459/18459 [00:16<00:00, 1144.67it/s]\n",
            "100%|██████████| 18459/18459 [00:16<00:00, 1128.27it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvC8oBPnfMOB",
        "outputId": "ffe37899-b967-43fe-e630-3b22b5071884"
      },
      "source": [
        "# Test the Toxic comment model on multiclass Slovenian Twitter dataset\n",
        "\n",
        "# Get the model and move it to GPU\n",
        "model = get_bert_crosloen(\"/content/drive/MyDrive/offensive_language_classification/crosloen_bert/\", \n",
        "                          pretrained_weights_path=\"/content/drive/MyDrive/offensive_language_classification/final_models/crosloen_toxic_comment_trained.pth\",\n",
        "                          num_labels=4)\n",
        "model.cuda()\n",
        "\n",
        "# Run test\n",
        "test(test_dataloader, model, score_average=\"macro\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/offensive_language_classification/crosloen_bert/ were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/offensive_language_classification/crosloen_bert/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.24047889918197085, f1-score: 0.17136136084428483, precision: 0.24734685719052849, recall: 0.24948688666552163\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.24047889918197085,\n",
              " 0.17136136084428483,\n",
              " 0.24734685719052849,\n",
              " 0.24948688666552163)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PStoLwMvgNPZ",
        "outputId": "3c911e70-b548-4643-dba6-be15db717c08"
      },
      "source": [
        "# Test the Gab binary model on binarized Slovenian Twitter dataset\n",
        "# Get the model and move it to GPU\n",
        "model = get_bert_crosloen(\"/content/drive/MyDrive/offensive_language_classification/crosloen_bert/\", \n",
        "                          pretrained_weights_path=\"/content/drive/MyDrive/offensive_language_classification/final_models/crosloen_gab_trained.pth\")\n",
        "model.cuda()\n",
        "\n",
        "# Run test\n",
        "test(test_dataloader_binary, model, score_average=\"binary\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/offensive_language_classification/crosloen_bert/ were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/offensive_language_classification/crosloen_bert/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.49618072484966685, f1-score: 0.045174537987679675, precision: 0.6077348066298343, recall: 0.02345915973555129\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.49618072484966685,\n",
              " 0.045174537987679675,\n",
              " 0.6077348066298343,\n",
              " 0.02345915973555129)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIPOwoGwg1Cr",
        "outputId": "c0783149-ed21-4b13-b7d4-eb6be222a2ee"
      },
      "source": [
        "# Test the Reddit binary model on binarized Slovenian Twitter dataset\n",
        "# Get the model and move it to GPU\n",
        "model = get_bert_crosloen(\"/content/drive/MyDrive/offensive_language_classification/crosloen_bert/\", \n",
        "                          pretrained_weights_path=\"/content/drive/MyDrive/offensive_language_classification/final_models/crosloen_reddit_trained.pth\",\n",
        "                          num_labels=2)\n",
        "model.cuda()\n",
        "\n",
        "# Run test\n",
        "test(test_dataloader_binary, model, score_average=\"binary\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/offensive_language_classification/crosloen_bert/ were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/offensive_language_classification/crosloen_bert/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.4973725553930332, f1-score: 0.05519348268839104, precision: 0.6131221719457014, recall: 0.028897419492429088\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.4973725553930332,\n",
              " 0.05519348268839104,\n",
              " 0.6131221719457014,\n",
              " 0.028897419492429088)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HisMvSuImxRK"
      },
      "source": [
        "# **Monolingual BERT training on ENG datasets and testing on ENG and translated SLO datasets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWzwSiLp4AgB"
      },
      "source": [
        "## Binary datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LkLfhwd4AgD"
      },
      "source": [
        "### Gab training and testing on ENG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M83mT6hX4AgF",
        "outputId": "a4360da3-f299-4149-9662-2b5e2a67e1c3"
      },
      "source": [
        "# Training\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 2e-5\n",
        "epochs = 3\n",
        "max_length = 128\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/offensive_language_classification/final_datasets/gab/train.csv\")# (17870 false, 146001 true)\n",
        "comments = df[\"content\"].values\n",
        "labels = df[\"type\"].values\n",
        "\n",
        "# Get Pytorch device\n",
        "device = get_torch_device()\n",
        "\n",
        "# Prepare the training and validation data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "train_dataloader, val_dataloader = prepare_training_dataloaders(comments, labels, tokenizer, batch_size, max_length, device)\n",
        "\n",
        "# Get the model and move it to GPU\n",
        "model = get_bert_eng()\n",
        "model.cuda()\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr=learning_rate,  # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps=1e-8  # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(train_dataloader) * epochs)\n",
        "\n",
        "state_dict = train(model, optimizer, scheduler, train_dataloader, val_dataloader, epochs, device)\n",
        "torch.save(state_dict, \"/content/drive/MyDrive/offensive_language_classification/final_models/bert_eng_gab_trained.pth\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: Tesla K80\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/20783 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 20783/20783 [00:22<00:00, 942.10it/s] \n",
            "100%|██████████| 5196/5196 [00:05<00:00, 949.84it/s] \n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    650.    Elapsed: 0:01:11.\n",
            "  Batch    80  of    650.    Elapsed: 0:02:20.\n",
            "  Batch   120  of    650.    Elapsed: 0:03:30.\n",
            "  Batch   160  of    650.    Elapsed: 0:04:40.\n",
            "  Batch   200  of    650.    Elapsed: 0:05:50.\n",
            "  Batch   240  of    650.    Elapsed: 0:07:00.\n",
            "  Batch   280  of    650.    Elapsed: 0:08:09.\n",
            "  Batch   320  of    650.    Elapsed: 0:09:19.\n",
            "  Batch   360  of    650.    Elapsed: 0:10:29.\n",
            "  Batch   400  of    650.    Elapsed: 0:11:39.\n",
            "  Batch   440  of    650.    Elapsed: 0:12:48.\n",
            "  Batch   480  of    650.    Elapsed: 0:13:58.\n",
            "  Batch   520  of    650.    Elapsed: 0:15:07.\n",
            "  Batch   560  of    650.    Elapsed: 0:16:17.\n",
            "  Batch   600  of    650.    Elapsed: 0:17:27.\n",
            "  Batch   640  of    650.    Elapsed: 0:18:37.\n",
            "\n",
            "  Average training loss: 0.27\n",
            "  Training epoch took: 0:18:54\n",
            "\n",
            "Running Validation...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:204: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:205: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Accuracy: 0.93\n",
            "f1-score: 0.92, precision: 0.94, recall: 0.9\n",
            "  Validation Loss: 0.21\n",
            "  Validation took: 0:01:15\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    650.    Elapsed: 0:01:10.\n",
            "  Batch    80  of    650.    Elapsed: 0:02:20.\n",
            "  Batch   120  of    650.    Elapsed: 0:03:30.\n",
            "  Batch   160  of    650.    Elapsed: 0:04:40.\n",
            "  Batch   200  of    650.    Elapsed: 0:05:50.\n",
            "  Batch   240  of    650.    Elapsed: 0:07:00.\n",
            "  Batch   280  of    650.    Elapsed: 0:08:10.\n",
            "  Batch   320  of    650.    Elapsed: 0:09:19.\n",
            "  Batch   360  of    650.    Elapsed: 0:10:29.\n",
            "  Batch   400  of    650.    Elapsed: 0:11:39.\n",
            "  Batch   440  of    650.    Elapsed: 0:12:49.\n",
            "  Batch   480  of    650.    Elapsed: 0:13:59.\n",
            "  Batch   520  of    650.    Elapsed: 0:15:09.\n",
            "  Batch   560  of    650.    Elapsed: 0:16:19.\n",
            "  Batch   600  of    650.    Elapsed: 0:17:29.\n",
            "  Batch   640  of    650.    Elapsed: 0:18:39.\n",
            "\n",
            "  Average training loss: 0.19\n",
            "  Training epoch took: 0:18:55\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.93\n",
            "f1-score: 0.93, precision: 0.93, recall: 0.92\n",
            "  Validation Loss: 0.20\n",
            "  Validation took: 0:01:15\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    650.    Elapsed: 0:01:10.\n",
            "  Batch    80  of    650.    Elapsed: 0:02:20.\n",
            "  Batch   120  of    650.    Elapsed: 0:03:30.\n",
            "  Batch   160  of    650.    Elapsed: 0:04:39.\n",
            "  Batch   200  of    650.    Elapsed: 0:05:49.\n",
            "  Batch   240  of    650.    Elapsed: 0:06:58.\n",
            "  Batch   280  of    650.    Elapsed: 0:08:07.\n",
            "  Batch   320  of    650.    Elapsed: 0:09:16.\n",
            "  Batch   360  of    650.    Elapsed: 0:10:25.\n",
            "  Batch   400  of    650.    Elapsed: 0:11:34.\n",
            "  Batch   440  of    650.    Elapsed: 0:12:43.\n",
            "  Batch   480  of    650.    Elapsed: 0:13:52.\n",
            "  Batch   520  of    650.    Elapsed: 0:15:01.\n",
            "  Batch   560  of    650.    Elapsed: 0:16:10.\n",
            "  Batch   600  of    650.    Elapsed: 0:17:19.\n",
            "  Batch   640  of    650.    Elapsed: 0:18:28.\n",
            "\n",
            "  Average training loss: 0.14\n",
            "  Training epoch took: 0:18:45\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.93\n",
            "f1-score: 0.92, precision: 0.92, recall: 0.92\n",
            "  Validation Loss: 0.24\n",
            "  Validation took: 0:01:14\n",
            "\n",
            "Training complete!\n",
            "Total training took 1:00:17 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrNT2cnW4AgI",
        "outputId": "c47e95a1-0a22-40c9-aba7-1b38f823d010"
      },
      "source": [
        "# Testing\n",
        "max_length = 128\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/offensive_language_classification/final_datasets/gab/test.csv\")# (17870 false, 146001 true)\n",
        "comments = df[\"content\"].values\n",
        "labels = df[\"type\"].values\n",
        "\n",
        "# Get Pytorch device\n",
        "device = get_torch_device()\n",
        "\n",
        "# Prepare the training and validation data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "test_dataloader = prepare_test_dataloader(comments, labels, tokenizer, batch_size, max_length, device)\n",
        "\n",
        "# Get the model and move it to GPU\n",
        "model = get_bert_eng(pretrained_weights_path=\"/content/drive/MyDrive/offensive_language_classification/final_models/bert_eng_gab_trained.pth\")\n",
        "model.cuda()\n",
        "\n",
        "# Run test\n",
        "test(test_dataloader, model, score_average=\"binary\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: Tesla K80\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/6495 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 6495/6495 [00:06<00:00, 1014.68it/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.9217859892224788, f1-score: 0.9110955547777388, precision: 0.9130129779024904, recall: 0.9091861683548725\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9217859892224788,\n",
              " 0.9110955547777388,\n",
              " 0.9130129779024904,\n",
              " 0.9091861683548725)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6YNMf3w4AgJ"
      },
      "source": [
        "### Reddit training and testing on ENG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8oP0uX84AgJ",
        "outputId": "3bf15c9e-21b9-42d3-cb09-ce741a84f6fb"
      },
      "source": [
        "# Training\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 2e-5\n",
        "epochs = 3\n",
        "max_length = 128\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/offensive_language_classification/final_datasets/reddit/train.csv\")# (17870 false, 146001 true)\n",
        "comments = df[\"content\"].values\n",
        "labels = df[\"type\"].values\n",
        "\n",
        "# Get Pytorch device\n",
        "device = get_torch_device()\n",
        "\n",
        "# Prepare the training and validation data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "train_dataloader, val_dataloader = prepare_training_dataloaders(comments, labels, tokenizer, batch_size, max_length, device)\n",
        "\n",
        "# Get the model and move it to GPU\n",
        "model = get_bert_eng()\n",
        "model.cuda()\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr=learning_rate,  # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps=1e-8  # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(train_dataloader) * epochs)\n",
        "\n",
        "state_dict = train(model, optimizer, scheduler, train_dataloader, val_dataloader, epochs, device)\n",
        "torch.save(state_dict, \"/content/drive/MyDrive/offensive_language_classification/final_models/bert_eng_reddit_trained.pth\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: Tesla K80\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/14214 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 14214/14214 [00:20<00:00, 703.28it/s]\n",
            "100%|██████████| 3554/3554 [00:05<00:00, 665.71it/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    445.    Elapsed: 0:01:10.\n",
            "  Batch    80  of    445.    Elapsed: 0:02:19.\n",
            "  Batch   120  of    445.    Elapsed: 0:03:29.\n",
            "  Batch   160  of    445.    Elapsed: 0:04:38.\n",
            "  Batch   200  of    445.    Elapsed: 0:05:47.\n",
            "  Batch   240  of    445.    Elapsed: 0:06:57.\n",
            "  Batch   280  of    445.    Elapsed: 0:08:06.\n",
            "  Batch   320  of    445.    Elapsed: 0:09:16.\n",
            "  Batch   360  of    445.    Elapsed: 0:10:25.\n",
            "  Batch   400  of    445.    Elapsed: 0:11:34.\n",
            "  Batch   440  of    445.    Elapsed: 0:12:44.\n",
            "\n",
            "  Average training loss: 0.29\n",
            "  Training epoch took: 0:12:51\n",
            "\n",
            "Running Validation...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:204: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:205: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Accuracy: 0.91\n",
            "f1-score: 0.81, precision: 0.81, recall: 0.81\n",
            "  Validation Loss: 0.25\n",
            "  Validation took: 0:00:50\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    445.    Elapsed: 0:01:09.\n",
            "  Batch    80  of    445.    Elapsed: 0:02:19.\n",
            "  Batch   120  of    445.    Elapsed: 0:03:28.\n",
            "  Batch   160  of    445.    Elapsed: 0:04:38.\n",
            "  Batch   200  of    445.    Elapsed: 0:05:47.\n",
            "  Batch   240  of    445.    Elapsed: 0:06:56.\n",
            "  Batch   280  of    445.    Elapsed: 0:08:06.\n",
            "  Batch   320  of    445.    Elapsed: 0:09:15.\n",
            "  Batch   360  of    445.    Elapsed: 0:10:24.\n",
            "  Batch   400  of    445.    Elapsed: 0:11:34.\n",
            "  Batch   440  of    445.    Elapsed: 0:12:43.\n",
            "\n",
            "  Average training loss: 0.21\n",
            "  Training epoch took: 0:12:51\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91\n",
            "f1-score: 0.81, precision: 0.8, recall: 0.82\n",
            "  Validation Loss: 0.27\n",
            "  Validation took: 0:00:50\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    445.    Elapsed: 0:01:09.\n",
            "  Batch    80  of    445.    Elapsed: 0:02:19.\n",
            "  Batch   120  of    445.    Elapsed: 0:03:28.\n",
            "  Batch   160  of    445.    Elapsed: 0:04:38.\n",
            "  Batch   200  of    445.    Elapsed: 0:05:47.\n",
            "  Batch   240  of    445.    Elapsed: 0:06:56.\n",
            "  Batch   280  of    445.    Elapsed: 0:08:06.\n",
            "  Batch   320  of    445.    Elapsed: 0:09:15.\n",
            "  Batch   360  of    445.    Elapsed: 0:10:25.\n",
            "  Batch   400  of    445.    Elapsed: 0:11:34.\n",
            "  Batch   440  of    445.    Elapsed: 0:12:43.\n",
            "\n",
            "  Average training loss: 0.17\n",
            "  Training epoch took: 0:12:51\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91\n",
            "f1-score: 0.81, precision: 0.79, recall: 0.83\n",
            "  Validation Loss: 0.27\n",
            "  Validation took: 0:00:50\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:41:04 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdshEEZD4AgK",
        "outputId": "cbcbf9ec-e6b5-4400-927d-f3f041502298"
      },
      "source": [
        "# Testing\n",
        "max_length = 128\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/offensive_language_classification/final_datasets/reddit/test.csv\")# (17870 false, 146001 true)\n",
        "comments = df[\"content\"].values\n",
        "labels = df[\"type\"].values\n",
        "\n",
        "\n",
        "# Get Pytorch device\n",
        "device = get_torch_device()\n",
        "\n",
        "# Prepare the training and validation data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "test_dataloader = prepare_test_dataloader(comments, labels, tokenizer, batch_size, max_length, device)\n",
        "\n",
        "# Get the model and move it to GPU\n",
        "model = get_bert_eng(pretrained_weights_path=\"/content/drive/MyDrive/offensive_language_classification/final_models/bert_eng_reddit_trained.pth\")\n",
        "model.cuda()\n",
        "\n",
        "# Run test\n",
        "test(test_dataloader, model, score_average=\"macro\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: Tesla K80\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/4442 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 4442/4442 [00:06<00:00, 667.45it/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.9124268347591176, f1-score: 0.8816573145423792, precision: 0.8785303259180368, recall: 0.8849259226003412\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9124268347591176,\n",
              " 0.8816573145423792,\n",
              " 0.8785303259180368,\n",
              " 0.8849259226003412)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj50sPCdl9RB"
      },
      "source": [
        "## Multi-class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE-mJTdEl_L_"
      },
      "source": [
        "### Kaggle toxic comment training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkX_ISF7l_MB",
        "outputId": "a5f9e6c9-bf40-4ec7-f34c-256ba088f713"
      },
      "source": [
        "# Training\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 2e-5\n",
        "epochs = 3\n",
        "max_length = 128\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/offensive_language_classification/final_datasets/toxic_comment_relabeled_multiclass/train.csv\")\n",
        "comments = df[\"content\"].values\n",
        "labels = df[\"type\"].values\n",
        "\n",
        "# Get Pytorch device\n",
        "device = get_torch_device()\n",
        "\n",
        "# Prepare the training and validation data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "train_dataloader, val_dataloader = prepare_training_dataloaders(comments, labels, tokenizer, batch_size, max_length, device)\n",
        "\n",
        "# Get the model and move it to GPU\n",
        "model = get_bert_eng(num_labels=4)\n",
        "model.cuda()\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr=learning_rate,  # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps=1e-8  # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(train_dataloader) * epochs)\n",
        "\n",
        "state_dict = train(model, optimizer, scheduler, train_dataloader, val_dataloader, epochs, device, multiclass=True)\n",
        "torch.save(state_dict, \"/content/drive/MyDrive/offensive_language_classification/final_models/bert_eng_toxic_comment_trained.pth\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: Tesla K80\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/15121 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 15121/15121 [00:30<00:00, 492.90it/s]\n",
            "100%|██████████| 3781/3781 [00:07<00:00, 487.89it/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    473.    Elapsed: 0:01:11.\n",
            "  Batch    80  of    473.    Elapsed: 0:02:21.\n",
            "  Batch   120  of    473.    Elapsed: 0:03:31.\n",
            "  Batch   160  of    473.    Elapsed: 0:04:41.\n",
            "  Batch   200  of    473.    Elapsed: 0:05:52.\n",
            "  Batch   240  of    473.    Elapsed: 0:07:02.\n",
            "  Batch   280  of    473.    Elapsed: 0:08:12.\n",
            "  Batch   320  of    473.    Elapsed: 0:09:22.\n",
            "  Batch   360  of    473.    Elapsed: 0:10:31.\n",
            "  Batch   400  of    473.    Elapsed: 0:11:41.\n",
            "  Batch   440  of    473.    Elapsed: 0:12:51.\n",
            "\n",
            "  Average training loss: 0.67\n",
            "  Training epoch took: 0:13:48\n",
            "\n",
            "Running Validation...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:204: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:205: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Accuracy: 0.75\n",
            "f1-score: 0.75, precision: 0.75, recall: 0.75\n",
            "  Validation Loss: 0.58\n",
            "  Validation took: 0:00:54\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    473.    Elapsed: 0:01:10.\n",
            "  Batch    80  of    473.    Elapsed: 0:02:20.\n",
            "  Batch   120  of    473.    Elapsed: 0:03:30.\n",
            "  Batch   160  of    473.    Elapsed: 0:04:40.\n",
            "  Batch   200  of    473.    Elapsed: 0:05:50.\n",
            "  Batch   240  of    473.    Elapsed: 0:07:01.\n",
            "  Batch   280  of    473.    Elapsed: 0:08:11.\n",
            "  Batch   320  of    473.    Elapsed: 0:09:21.\n",
            "  Batch   360  of    473.    Elapsed: 0:10:31.\n",
            "  Batch   400  of    473.    Elapsed: 0:11:41.\n",
            "  Batch   440  of    473.    Elapsed: 0:12:51.\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epoch took: 0:13:48\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.77\n",
            "f1-score: 0.77, precision: 0.77, recall: 0.77\n",
            "  Validation Loss: 0.54\n",
            "  Validation took: 0:00:54\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    473.    Elapsed: 0:01:10.\n",
            "  Batch    80  of    473.    Elapsed: 0:02:20.\n",
            "  Batch   120  of    473.    Elapsed: 0:03:30.\n",
            "  Batch   160  of    473.    Elapsed: 0:04:40.\n",
            "  Batch   200  of    473.    Elapsed: 0:05:50.\n",
            "  Batch   240  of    473.    Elapsed: 0:07:00.\n",
            "  Batch   280  of    473.    Elapsed: 0:08:11.\n",
            "  Batch   320  of    473.    Elapsed: 0:09:21.\n",
            "  Batch   360  of    473.    Elapsed: 0:10:31.\n",
            "  Batch   400  of    473.    Elapsed: 0:11:41.\n",
            "  Batch   440  of    473.    Elapsed: 0:12:51.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0:13:48\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.77\n",
            "f1-score: 0.77, precision: 0.77, recall: 0.77\n",
            "  Validation Loss: 0.58\n",
            "  Validation took: 0:00:55\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:44:08 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFA8x4t7l_MD",
        "outputId": "70c0a77e-4743-4c06-85d2-7e49aad0732b"
      },
      "source": [
        "# Testing\n",
        "max_length = 128\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/offensive_language_classification/final_datasets/toxic_comment_relabeled_multiclass/test.csv\")# (17870 false, 146001 true)\n",
        "comments = df[\"content\"].values\n",
        "labels = df[\"type\"].values\n",
        "\n",
        "\n",
        "# Get Pytorch device\n",
        "device = get_torch_device()\n",
        "\n",
        "# Prepare the training and validation data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "test_dataloader = prepare_test_dataloader(comments, labels, tokenizer, batch_size, max_length, device)\n",
        "\n",
        "# Get the model and move it to GPU\n",
        "model = get_bert_eng(pretrained_weights_path=\"/content/drive/MyDrive/offensive_language_classification/final_models/bert_eng_toxic_comment_trained.pth\",\n",
        "                          num_labels=4)\n",
        "model.cuda()\n",
        "\n",
        "# Run test\n",
        "test(test_dataloader, model, score_average=\"macro\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: Tesla K80\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/4823 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 4823/4823 [00:09<00:00, 513.44it/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.7851959361393324, f1-score: 0.7244239088784382, precision: 0.7198556894524715, recall: 0.730058302959308\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7851959361393324, 0.7244239088784382, 0.7198556894524715, 0.730058302959308)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eFOY6WhUw-7"
      },
      "source": [
        "### Trac2 Subtask A training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU6oo48JUw-9",
        "outputId": "8d76627d-b112-4e46-d148-d90fa2ac0d41"
      },
      "source": [
        "# Training\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 2e-5\n",
        "epochs = 3\n",
        "max_length = 128\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/offensive_language_classification/final_datasets/trac2/train.csv\")\n",
        "comments = df[\"content\"].fillna(\"\").values\n",
        "labels = df[\"type\"].fillna(0).values\n",
        "\n",
        "# Get Pytorch device\n",
        "device = get_torch_device()\n",
        "\n",
        "# Prepare the training and validation data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "train_dataloader, val_dataloader = prepare_training_dataloaders(comments, labels, tokenizer, batch_size, max_length, device)\n",
        "\n",
        "# Get the model and move it to GPU\n",
        "model = get_bert_eng(num_labels=3)\n",
        "model.cuda()\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr=learning_rate,  # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps=1e-8  # args.adam_epsilon  - default is 1e-8.\n",
        "                  )\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(train_dataloader) * epochs)\n",
        "\n",
        "state_dict = train(model, optimizer, scheduler, train_dataloader, val_dataloader, epochs, device, multiclass=True)\n",
        "torch.save(state_dict, \"/content/drive/MyDrive/offensive_language_classification/final_models/bert_eng_trac2_trained.pth\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: Tesla K80\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/3410 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 3410/3410 [00:02<00:00, 1427.27it/s]\n",
            "100%|██████████| 853/853 [00:00<00:00, 1234.79it/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    107.    Elapsed: 0:01:09.\n",
            "  Batch    80  of    107.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.60\n",
            "  Training epoch took: 0:03:04\n",
            "\n",
            "Running Validation...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:204: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:205: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Accuracy: 0.77\n",
            "f1-score: 0.77, precision: 0.77, recall: 0.77\n",
            "  Validation Loss: 0.54\n",
            "  Validation took: 0:00:12\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    107.    Elapsed: 0:01:09.\n",
            "  Batch    80  of    107.    Elapsed: 0:02:17.\n",
            "\n",
            "  Average training loss: 0.45\n",
            "  Training epoch took: 0:03:03\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "f1-score: 0.81, precision: 0.81, recall: 0.81\n",
            "  Validation Loss: 0.46\n",
            "  Validation took: 0:00:12\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    107.    Elapsed: 0:01:09.\n",
            "  Batch    80  of    107.    Elapsed: 0:02:18.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0:03:03\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "f1-score: 0.8, precision: 0.8, recall: 0.8\n",
            "  Validation Loss: 0.48\n",
            "  Validation took: 0:00:12\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:09:46 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atAIyYuHUw_A",
        "outputId": "966b2285-abb7-451b-ac24-db05427a24d4"
      },
      "source": [
        "# Testing\n",
        "max_length = 128\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/offensive_language_classification/final_datasets/trac2/test.csv\")# (17870 false, 146001 true)\n",
        "comments = df[\"content\"].values\n",
        "labels = df[\"type\"].values\n",
        "\n",
        "\n",
        "# Get Pytorch device\n",
        "device = get_torch_device()\n",
        "\n",
        "# Prepare the training and validation data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "test_dataloader = prepare_test_dataloader(comments, labels, tokenizer, batch_size, max_length, device)\n",
        "\n",
        "# Get the model and move it to GPU\n",
        "model = get_bert_eng(pretrained_weights_path=\"/content/drive/MyDrive/offensive_language_classification/final_models/bert_eng_trac2_trained.pth\",\n",
        "                          num_labels=3)\n",
        "model.cuda()\n",
        "\n",
        "# Run test\n",
        "test(test_dataloader, model, score_average=\"macro\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: Tesla K80\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1066 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 1066/1066 [00:00<00:00, 1345.13it/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.8142589118198874, f1-score: 0.5586655082660529, precision: 0.6197413498029612, recall: 0.5388265774629412\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8142589118198874,\n",
              " 0.5586655082660529,\n",
              " 0.6197413498029612,\n",
              " 0.5388265774629412)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTFVqSeEbi0D"
      },
      "source": [
        "## Testing on Slovenian translated data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BcP7ZUhbrmW",
        "outputId": "d0fb831e-dfb0-4fd1-aecf-338b82dd2081"
      },
      "source": [
        "# Testing\n",
        "max_length = 128\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/offensive_language_classification/final_datasets/slo-twitter-dataset-translated.csv\")# (17870 false, 146001 true)\n",
        "comments = df[\"content\"].values\n",
        "labels = df[\"type\"].values\n",
        "\n",
        "\n",
        "# Get Pytorch device\n",
        "device = get_torch_device()\n",
        "\n",
        "# Prepare the training and validation data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "test_dataloader = prepare_test_dataloader(comments, labels, tokenizer, batch_size, max_length, device)\n",
        "\n",
        "# Get the model and move it to GPU\n",
        "model = get_bert_eng(pretrained_weights_path=\"/content/drive/MyDrive/offensive_language_classification/final_models/bert_eng_toxic_comment_trained.pth\",\n",
        "                          num_labels=4)\n",
        "model.cuda()\n",
        "\n",
        "# Run test\n",
        "test(test_dataloader, model, score_average=\"macro\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: Tesla K80\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/18459 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 18459/18459 [00:14<00:00, 1294.83it/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.3891326724091229, f1-score: 0.23459841879788543, precision: 0.31249486479113586, recall: 0.32971637562382733\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3891326724091229,\n",
              " 0.23459841879788543,\n",
              " 0.31249486479113586,\n",
              " 0.32971637562382733)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJhT3yPggF8I",
        "outputId": "28a51c2b-1162-4983-9d84-a394e09efdd5"
      },
      "source": [
        "# Preparation for testing\n",
        "max_length = 128\n",
        "batch_size = 32\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/offensive_language_classification/final_datasets/slo-twitter-dataset-translated.csv\")# (17870 false, 146001 true)\n",
        "comments = df[\"content\"].values\n",
        "labels = df[\"type\"].values\n",
        "\n",
        "\n",
        "# Get Pytorch device\n",
        "device = get_torch_device()\n",
        "\n",
        "# Prepare the training and validation data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "# test_dataloader = prepare_test_dataloader(comments, labels, tokenizer, batch_size, max_length, device)\n",
        "\n",
        "labels_binary = np.minimum(labels, 1)\n",
        "test_dataloader_binary = prepare_test_dataloader(comments, labels_binary, tokenizer, batch_size, max_length, device)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: Tesla K80\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/18459 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 18459/18459 [00:14<00:00, 1284.27it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS7oKWILgZ1A",
        "outputId": "2b7c75ed-7808-4d6b-fb5c-d5ab2f6352d2"
      },
      "source": [
        "\n",
        "\n",
        "# Get the model and move it to GPU\n",
        "model = get_bert_eng(pretrained_weights_path=\"/content/drive/MyDrive/offensive_language_classification/final_models/bert_eng_gab_trained.pth\",\n",
        "                          num_labels=2)\n",
        "model.cuda()\n",
        "\n",
        "# Run test\n",
        "test(test_dataloader_binary, model, score_average=\"binary\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.4991603012080828, f1-score: 0.039480519480519484, precision: 0.7692307692307693, recall: 0.020260183407976116\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.4991603012080828,\n",
              " 0.039480519480519484,\n",
              " 0.7692307692307693,\n",
              " 0.020260183407976116)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlS3UtQykNOF",
        "outputId": "0cffd16a-b78f-4d51-d7c8-6c85bb260275"
      },
      "source": [
        "\n",
        "\n",
        "# Get the model and move it to GPU\n",
        "model = get_bert_eng(pretrained_weights_path=\"/content/drive/MyDrive/offensive_language_classification/final_models/bert_eng_reddit_trained.pth\",\n",
        "                          num_labels=2)\n",
        "model.cuda()\n",
        "\n",
        "# Run test\n",
        "test(test_dataloader_binary, model, score_average=\"binary\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.49715585893060293, f1-score: 0.026431718061674006, precision: 0.8076923076923077, recall: 0.013435700575815739\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.49715585893060293,\n",
              " 0.026431718061674006,\n",
              " 0.8076923076923077,\n",
              " 0.013435700575815739)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}